{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 7/7 [19:31<00:00, 167.35s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 68.34it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "template = \"\"\" Question : {question}\n",
    "Answer : Let's thin it setp by step.\"\"\"\n",
    "\n",
    "prompt= PromptTemplate(template = template, input_variables= [\"question\"])\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'allenai/Molmo-7B-D-0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramachandra/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunk: 65517\n",
      "Sample keys: dict_keys(['input_ids', 'attention_mask'])\n",
      "Input IDs shape: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict\n",
    "\n",
    "class WikipediaDataset(Dataset):\n",
    "    def __init__(self, tokenizer, chunk_size=512, chunk_overlap=24, subset_size=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        # Load dataset\n",
    "        dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "        if subset_size is not None:\n",
    "            dataset = dataset.select(range(min(subset_size, len(dataset))))\n",
    "        \n",
    "        # Split all texts and flatten the list\n",
    "        self.chunks: List[str] = []\n",
    "        for text in dataset[\"text\"]:\n",
    "            text_chunks = self.text_splitter.split_text(text)\n",
    "            self.chunks.extend(text_chunks)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Dict:\n",
    "        # Tokenize the chunk\n",
    "        encodings = self.tokenizer(\n",
    "            self.chunks[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.chunk_size,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove the batch dimension added by the tokenizer\n",
    "        return {key: value.squeeze(0) for key, value in encodings.items()}\n",
    "\n",
    "# Example usage:\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = WikipediaDataset(tokenizer, chunk_size=512, chunk_overlap=24, subset_size=1000)\n",
    "print(f\"Total chunk: {len(dataset)}\")\n",
    "\n",
    "# Get a sample\n",
    "sample = dataset[0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramachandra/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 65517\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 1\n",
      "\n",
      "ORIGINAL TEXT (first 100 chars):\n",
      "Anarchism is a political philosophy and movement that is sceptical of authority and rejects all invo len= 43985...\n",
      "\n",
      "SPLIT INTO 133 CHUNKS:\n",
      "\n",
      "Chunk 1 (511 chars):\n",
      "Anarchism is a political philosophy and movement that is sceptical of authority and rejects all invo...\n",
      "\n",
      "Chunk 2 (71 chars):\n",
      "has a strong historical association with anti-capitalism and socialism.\n",
      "\n",
      "Chunk 3 (511 chars):\n",
      "Humans lived in societies without formal hierarchies long before the establishment of formal states,...\n",
      "\n",
      "... (skipping middle chunks) ...\n",
      "\n",
      "Last Chunk (207 chars):\n",
      "Anti-capitalism\n",
      "Anti-fascism\n",
      "Economic ideologies\n",
      "Left-wing politics\n",
      "Libertarian socialism\n",
      "Libertaria...\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 2\n",
      "\n",
      "ORIGINAL TEXT (first 100 chars):\n",
      "Autism is a neurodevelopmental disorder characterized by difficulties with social interaction and co len= 46773...\n",
      "\n",
      "SPLIT INTO 138 CHUNKS:\n",
      "\n",
      "Chunk 1 (422 chars):\n",
      "Autism is a neurodevelopmental disorder characterized by difficulties with social interaction and co...\n",
      "\n",
      "Chunk 2 (511 chars):\n",
      "Autism is associated with a combination of genetic and environmental factors. Risk factors during pr...\n",
      "\n",
      "Chunk 3 (328 chars):\n",
      "their synapses connect and organize; how this occurs is not well understood. The Diagnostic and Stat...\n",
      "\n",
      "... (skipping middle chunks) ...\n",
      "\n",
      "Last Chunk (218 chars):\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      " \n",
      "1910s neologisms\n",
      "Articles containing video clips\n",
      "Communication disorde...\n",
      "\n",
      "================================================================================\n",
      "Sample keys: dict_keys(['input_ids', 'attention_mask'])\n",
      "Input IDs shape: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict\n",
    "\n",
    "class WikipediaDataset(Dataset):\n",
    "    def __init__(self, tokenizer, chunk_size=512, chunk_overlap=24, subset_size=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        # Load dataset\n",
    "        dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "        if subset_size is not None:\n",
    "            dataset = dataset.select(range(min(subset_size, len(dataset))))\n",
    "        \n",
    "        # Split all texts and flatten the list\n",
    "        self.chunks: List[str] = []\n",
    "        self.original_texts: List[str] = []  # Store original texts for demonstration\n",
    "        for text in dataset[\"text\"]:\n",
    "            text_chunks = self.text_splitter.split_text(text)\n",
    "            self.chunks.extend(text_chunks)\n",
    "            self.original_texts.append(text)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Dict:\n",
    "        # Tokenize the chunk\n",
    "        encodings = self.tokenizer(\n",
    "            self.chunks[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.chunk_size,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove the batch dimension added by the tokenizer\n",
    "        return {key: value.squeeze(0) for key, value in encodings.items()}\n",
    "\n",
    "    def print_sample_splits(self, num_samples=2):\n",
    "        for i in range(min(num_samples, len(self.original_texts))):\n",
    "            print(f\"\\n{'='*80}\\nSAMPLE {i+1}\")\n",
    "            print(f\"\\nORIGINAL TEXT (first 100 chars):\\n{self.original_texts[i][:100]} len= {len(self.original_texts[i])}...\")\n",
    "            \n",
    "            # Find chunks that came from this text\n",
    "            text_chunks = self.text_splitter.split_text(self.original_texts[i])\n",
    "            \n",
    "            print(f\"\\nSPLIT INTO {len(text_chunks)} CHUNKS:\")\n",
    "            for j, chunk in enumerate(text_chunks):\n",
    "                print(f\"\\nChunk {j+1} ({len(chunk)} chars):\")\n",
    "                print(f\"{chunk[:100]}...\" if len(chunk) > 100 else chunk)\n",
    "                \n",
    "                if j == 2 and len(text_chunks) > 3:  # If there are more chunks, just show the last one\n",
    "                    print(\"\\n... (skipping middle chunks) ...\")\n",
    "                    last_chunk = text_chunks[-1]\n",
    "                    print(f\"\\nLast Chunk ({len(last_chunk)} chars):\")\n",
    "                    print(f\"{last_chunk[:100]}...\" if len(last_chunk) > 100 else last_chunk)\n",
    "                    break\n",
    "\n",
    "# Example usage:\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = WikipediaDataset(tokenizer, chunk_size=512, chunk_overlap=24, subset_size=1000)\n",
    "print(f\"Total chunks: {len(dataset)}\")\n",
    "\n",
    "# Print sample splits\n",
    "dataset.print_sample_splits(num_samples=2)\n",
    "\n",
    "# Get a sample\n",
    "sample = dataset[0]\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
